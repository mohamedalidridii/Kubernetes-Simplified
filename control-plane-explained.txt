#
┌─────────────────────────────────────────────────────────────┐
│                      Control Plane Node                     │
│                                                             │
│  ┌──────────────┐                                           │
│  │   kubectl    │ (external client)                         │
│  └──────┬───────┘                                           │
│         │ HTTPS:6443                                        │
│         ▼                                                   │
│  ┌──────────────────────────────────────────┐               │
│  │        kube-apiserver                    │               │
│  │  (Central Hub - REST API Frontend)       │               │
│  └──┬────────┬──────────┬────────────────┬──┘               │
│     │        │          │                │                  │
│     │        │          │                │                  │
│     │ HTTPS  │ HTTPS    │ HTTPS          │ HTTPS            │
│     │ :2379  │ :10257   │ :10259         │ :10250           │
│     │        │          │                │                  │
│     ▼        ▼          ▼                ▼                  │
│  ┌─────┐ ┌─────────┐ ┌──────────┐    ┌────────┐             │
│  │etcd │ │controller││scheduler │    │kubelet │             │
│  │     │ │ manager  ││          │    │(worker)│             │
│  └─────┘ └─────────┘ └──────────┘    └────────┘             │
│    ▲                                                        │
│    │ Direct storage access                                  │
│    └────────────────────────────────────────────            │
└─────────────────────────────────────────────────────────────┘

#
1. etcd - The Database Layer:
Persistent key-value store for all cluster data


Technical Details:

Stores the entire cluster state (pods, services, secrets, configmaps, etc.)
Provides distributed consensus using Raft algorithm
Listens on:
	- Port 2379 for client connections (API server connects here)
	- Port 2380 for peer-to-peer communication (in multi-master setups)
	- Port 2381 for metrics

=> etcd is the only stateful component - if etcd data is lost, the entire cluster state is lost.

#

2. kube-apiserver - The Central Hub:
RESTful API frontend and gateway to the cluster

Technical Relationships:

Connection to etcd:
API Server → HTTPS:2379 → etcd

	- Only component that directly reads/writes to etcd
	- Uses client certificates for authentication (apiserver-etcd-client.crt/key)
	- All cluster state changes flow through: kubectl → API server → etcd
	- Implements optimistic concurrency control using resource versions

Connection to Controller Manager:
Controller Manger -> HTTPS:6443 -> API Server

	- Controller manager never talks to etcd directly
	- Uses /etc/kubernetes/controller-manager.conf kubeconfig to authenticate
	- Watch API: Controllers set up long-running watches on resources
		- Example: Deployment controller watches Deployment objects
		- API server notifies controllers of changes in real-time
	- Controllers read desired state and write back status updates

Connection to Scheduler:
Scheduler → HTTPS:6443 → API Server

	- Scheduler authenticates using /etc/kubernetes/scheduler.conf
	- Watch mechanism:
		- Watches for pods with spec.nodeName == "" (unscheduled pods)
		- Makes scheduling decision
		- Sends binding to API server: POST /api/v1/namespaces/{ns}/pods/{name}/binding
	- API server updates pod's spec.nodeName field in etcd

Connection to Kubelet (Worker Nodes): 
API Server ← → HTTPS:10250 ← → Kubelet

	- Bidirectional communication
	- API server uses apiserver-kubelet-client.crt/key for authentication
	- Used for: pod logs, exec, port-forward, metrics collection

#
3. kube-controller-manager - The Reconciliation Engine
Runs control loops that regulate cluster state

Technical Details:
How it Works:
	1- Watch desired state from API server
	2- Compare with actual state (also from API server)
	3- Reconcile by making API calls to fix driftA

Key Controllers and Their Interactions:

	- Node Controller:
		 Watch nodes → Detect unhealthy nodes → Mark as NotReady → 
		 Evict pods after grace period → API server updates pod status

	- ReplicaSet Controller:
		Watch ReplicaSets → Count running pods → 
		If fewer than desired: Create pods via API server →
		If more than desired: Delete pods via API server
	- Endpoints Controller:
		Watch Services & Pods → Match labels → 
		Update Endpoints object via API server
	- Service Account Controller:
		Watch Namespaces → Create default ServiceAccount →
		Watch ServiceAccounts → Create tokens/secrets

	- Leader Election:
		Flag: --leader-elect=true
		Uses lease objects in etcd (via API server)
		In multi-master: only one controller-manager actively runs
		Others are in standby, ready to take over within seconds

#
4. kube-scheduler - The Placement Engine
Assigns pods to nodes based on constraints and resources

Technical Details:
Scheduling Flow:
	1. Watch API server for pods where spec.nodeName == ""
	2. For each unscheduled pod:
   		a. Filtering phase: Eliminate unsuitable nodes
   		b. Scoring phase: Rank remaining nodes
   		c. Select best node
	3. Send binding to API server
	4. API server updates pod.spec.nodeName in etcd
	5. Kubelet on that node sees pod and starts ita

Filtering Predicates (Examples):
	- PodFitsResources: Node has enough CPU/memory
	- PodFitsHostPorts: Host ports are available
	- MatchNodeSelector: Node matches pod's nodeSelector
	- PodToleratesNodeTaints: Pod tolerates node taints

Scoring Functions (Examples):
	- LeastRequestedPriority: Prefers nodes with more available resources
	- BalancedResourceAllocation: Balances CPU and memory usage
	- InterPodAffinity: Respects pod affinity/anti-affinity rules

Leader Election:
	- Flag: --leader-elect=true
	- Only one scheduler is active in multi-master setups
	- If active scheduler fails, standby takes over

